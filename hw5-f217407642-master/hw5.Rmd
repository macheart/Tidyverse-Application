---
title: "MA [46]15 Homework 5"
author: "Prisma Lopez"
output: github_document
---

## Question 1

In this work I'll analyze the titles of papers presented at the NIPS
conference from 1987 to 2017. The data in file `nips-titles.csv`, loaded into
table `papers`, contains only two columns, `year` and `title` from the
[original Kaggle dataset](https://www.kaggle.com/benhamner/nips-papers).
As an initial check, I'm plotting the distribution of number of words in title
for each year.


**`[==[`**
Modify the `q1` chunk below to load needed libraries and load
`nips-titles.csv` into dataset `papers`. Check that `papers` has no problems.
Now create a new variable `word_count` with the number of words in each title
and then make boxplots of number of words per title for each year. Write a
short assessment about the variability of title sizes over the years.
**`]==]`**

```{r q1}
library(tidyverse)
library(tibble)
library(tidyr)
papers <- read_csv("data/nips-titles.csv")
papers
#problems(papers) # none

papers %>% 
  group_by(year) %>% 
  mutate(word_count = sapply(strsplit(title, " "), length)) %>%
  arrange(year) %>% 
  ggplot() + geom_boxplot(aes(x=year,y=word_count,group=year))
dim(papers) # 7241  2



```

## Question 2

```{r q2-info, echo = FALSE}
final <- tribble(
  ~year, ~bayesian, ~data, ~deep, ~learning, ~models, ~networks, ~neural, ~optimization, ~stochastic,
  2008L, 13L,       11L,   1L,    51L,       22L,     10L,       7L,      6L,            4L,         
  2009L, 20L,       9L,    4L,    61L,       24L,     6L,        7L,      7L,            6L,         
  2010L, 12L,       14L,   3L,    65L,       19L,     14L,       8L,      9L,            5L,         
  2011L, 9L,        15L,   3L,    79L,       25L,     9L,        11L,     10L,           10L,        
  2012L, 22L,       12L,   10L,   92L,       31L,     15L,       14L,     12L,           8L,         
  2013L, 20L,       18L,   13L,   77L,       33L,     18L,       17L,     17L,           17L,        
  2014L, 18L,       20L,   22L,   85L,       41L,     25L,       28L,     16L,           15L,        
  2015L, 16L,       16L,   21L,   78L,       36L,     33L,       22L,     31L,           22L,        
  2016L, 23L,       24L,   44L,   121L,      43L,     64L,       47L,     39L,           38L,        
  2017L, 20L,       35L,   64L,   180L,      38L,     69L,       64L,     29L,           25L
)
```


```{r q2}
keywords <- tibble(keyword = c("bayesian", "data", "deep", "models","networks", "neural", "learning", "optimization", "stochastic"))
# example of crossing between two tables:
# crossing(tibble(x = 1:3), tibble(y = letters[1:3]))

papers1 <- papers %>% 
  mutate(title = str_to_lower(title)) %>% # Alternative: mutate(title = tolower(title))
  data.frame() %>% # make it a dataframe (unecessary?)
  crossing(keywords)  %>% # cross words: produces a 
  group_by(year,keyword) %>%
  arrange(year,keyword) %>% 
  mutate(keyword_in_title = str_detect(title, keyword)) %>% # It is like you have gathered
  mutate(keyword_count = sum(keyword_in_title, na.rm=TRUE)) %>%
  spread(key = keyword, value = keyword_count) %>% 
  select(year, bayesian:stochastic) %>% 
  filter(!is.na(bayesian),
         !is.na(data),
         !is.na(deep),
         !is.na(learning),
         !is.na(models),
         !is.na(networks),
         !is.na(neural),
         !is.na(optimization),
         !is.na(stochastic)) %>%
  unique() %>% 
  filter(year >= 2008)


#papers2 <- unique(papers1[,1:10]) %>% 
  #filter(year >= 2008)
  
#install.packages('compare')
library(compare)
compare(papers1,final,allowAll = TRUE) # compare the two data frames

# OR
papers1 == final

```


## Question 3

**`[==[`**
It's hard to assess counts in a table, so, in a chunk labeled `q3`, make a
line plot of the counts you computed in Question 2, one line per keyword
(color each line by keyword). Comment on any specific patterns; for instance,
what seems to be the most popular keyword? Any keywords declining or
increasing in popularity?
**`]==]`**
```{r q3}
papers %>% 
  mutate(title = str_to_lower(title)) %>% # Alternative: mutate(title = tolower(title))
  data.frame() %>% # make it a dataframe (unecessary?)
  crossing(keywords)  %>% # cross words: produces a 
  group_by(year,keyword) %>%
  arrange(year,keyword) %>% 
  mutate(keyword_in_title = str_detect(title, keyword)) %>% # It is like you have gathered
  mutate(keyword_count = sum(keyword_in_title, na.rm=TRUE)) %>%
  ggplot() + geom_line(aes(x=year,y=keyword_count,color=keyword))


```


## Question 4



```{r q4}
# Calculate number of papers per 
papers_year <- papers %>% 
  group_by(year) %>%
  mutate(num_papers = n()) %>% 
  select(year,num_papers) %>% 
  unique() # notice, no input needed when you pipe the function

# Like in q2:
papeles <- papers %>% 
  mutate(title = str_to_lower(title)) %>% # Alternative: mutate(title = tolower(title))
  data.frame() %>% # make it a dataframe (unecessary?)
  crossing(keywords)  %>% # cross words: produces a 
  group_by(year,keyword) %>%
  arrange(year,keyword) %>% 
  mutate(keyword_in_title = str_detect(title, keyword)) %>% # It is like you have gathered
  mutate(keyword_count = sum(keyword_in_title, na.rm=TRUE)) %>%
  spread(key = keyword, value = keyword_count) %>% 
  select(year,title, bayesian:stochastic) %>% 
  filter(!is.na(bayesian),
         !is.na(data),
         !is.na(deep),
         !is.na(learning),
         !is.na(models),
         !is.na(networks),
         !is.na(neural),
         !is.na(optimization),
         !is.na(stochastic))

g <- gather(papeles,"bayesian","data","deep","learning","models","networks","neural","optimization","stochastic", key = keyword, value = keyword_count)

lj <- left_join(papers_year,g) # match all those with a year in papers_year

lj_p <- lj %>% 
  mutate(prop = keyword_count/num_papers) 
unique(lj_p)
ggplot(lj_p) + geom_line(aes(x=year,y=prop,color=keyword))

  
```
# Discussion

# Title word length fluctuates over the years from 1990 to 2018, where the title word length median is constant for several years (i.e. 1987,1988,1989,1990, 1991) followed by an increase in word length (2+ words) the subsequent year (i.e. 1992). Thereafter for several years, the title word count median goes back to the median value prior to the increase (i.e. 1993-1996). This trend is typical over multiple time frames. Only in 1997 is the title word length median lower in length compared to those for several years prior and for several years following. Note, 1997 marks a turning point in title word length median where from that year and for the years following up until 2017, is the most frequent title word count median for 1998 - 2017 less than that of the years 1987-1996.

# The most consistently included word in journal titles according to the data record is the keyword 'learning'. Since 1993, it has been more frequently included in titles than any of the other 8 keywords. Within the past five years, (2013-2018) do we observe a dramatic steep linear increase in that words usage. The keywords 'network', 'neural', and 'deep' are on the rise in title inclusion compared to five other keywords. This is given by their individual key word count in 2017 (networks: 69;neural: 64; deep: 64) as compared to previous years (i.e. 2015 networks: 33; neural:22; deep:21) and those of 5 other keywords (i.e. 2017 models: 38; data: 35)

# When the plot is modified to reflect the proportion of titles each keyword is included in per year, we can much more readily see that after 1993, the keyword 'learning' was found in greater frequency than any of the other keywords in the years following 1993+. You can also tell from the plot that the keyword 'neural' was the most popular (first to networks) and was included the most in word titles in 1987 than the other 8 keywords. However, since then, it lost popularity in the same manner that 'networks' has. Though, it has recently been gaining usage in titles at similar proportions as the 'networks' keyword. Notice the "dip" or "bowl" shape of the graph from 1997 - 2015 for those two keywords. From this graph, it is not so clear which (if any) of 7 keywords, not including learning, are on the rise (again) or not. 